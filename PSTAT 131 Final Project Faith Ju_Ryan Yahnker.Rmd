---
title: "PSTAT 131 Final Project"
author: "Faith Ju and Ryan Yahnker"
date: "`r Sys.Date()`"
output: pdf_document
---


# Introduction / Background Context 

Lung cancer is one of the most prevalent and deadliest forms of cancer worldwide, with over 2.2 million new cases and nearly 1.8 million deaths annually, according to the World Health Organization (WHO). It is the leading cause of cancer-related deaths, surpassing other forms of cancer. The disease is often diagnosed in its late stages, making treatment more difficult and significantly reducing survival rates. While smoking is the primary risk factor, lung cancer can also develop due to other factors like air pollution, occupational exposure to hazardous chemicals, and genetic factors. The disparities in access to healthcare and early screening amplify this issue, with many cases being diagnosed too late for effective treatment. 

In order to help identify early lung cancer risk, this model will help identify individuals who may be at a higher risk based on their lifestyle choices, health indicators, and environmental exposures. By analyzing key factors in this data set like smoking habits, exposure to pollution, family history, etc, the models we develop today will provide predictive insights that enable early intervention and preventative healthcare strategies. Early identification of high-risk individuals would allow for immediate medical screenings, adjustment of lifestyle habits, and possibly even targeted public health initiatives, with the goal of improving survival rates by detecting lung cancer at a more treatable stage. In a broader context, leveraging data-driven models like these can play a crucial role in shifting the focus from reactive treatments to proactive prevention, potentially reducing the burden of lung cancer on healthcare systems and saving lives.

# Data Description 

This dataset contains 5000 observations and 17 predictor variables, with PULMONARY_DISEASE being the response variable. These are the variables we will be using for our models. The predictors include demographic factors, lifestyle habits, and physiological indicators, all of which may contribute to an individual's risk of developing pulmonary disease.

- AGE: The age of each individual, ranging from 30-84 in this dataset. 
- GENDER: The gender of each individual, (1-Male, 0-Female)
- SMOKING: Whether or not each individual smokes (1-YES, 0-NO)
- FINGER_DISCOLORATION: Whether or not each individual experiences finger discoloration (1-YES, 0-NO)
- MENTAL_STRESS: Whether or not each individual experiences mental stress (1-YES, 0-NO)
- EXPOSURE_TO_POLLUTION: Whether or not each individual is exposed to pollution (1-YES, 0-NO)
- LONG_TERM_ILLNESS: Whether or not each individual is diagnosed with long-term-illness (1-YES, 0-NO)
- ENERGY_LEVEL: A measured assessment of overall vitality, with higher values indicating greater energy and lower values indicating fatigue or weakness, ranging from 0-100 
- IMMUNE_WEAKNESS: Whether or not an individual has a weak immune system (1-YES, 0-NO)
- BREATHING_ISSUE: Whether or not an individual experiences breathing issues (1-YES, 0-NO)
- ALCOHOL_CONSUMPTION: Whether or not an individual consumes alcohol (1-YES, 0-NO)
- THROAT_DISCOMFORT: Whether or not an individual experiences throat discomfort (1-YES, 0-NO)
- OXYGEN_SATURATION: The percentage of oxygen in an individual's bloodstream from 0-100, where lower levels may indicate respiratory issues or impaired lung function. 
- CHEST_TIGHTNESS: Whether or not an individual experiences chest tightness (1-YES, 0-NO)
- FAMILY_HISTORY: Whether or not an individual has a family history of lung cancer (1-YES, 0-NO)
- SMOKING_FAMILY_HISTORY: Whether or not an individual has a family history of smoking (1-YES, 0-NO)
- STRESS_IMMUNE: Whether or not stress has impacted an individual's immmune system (1-YES, 0-NO)
- PULMONARY_DISEASE: Whether or not an individual is diagnosed with pulmonary/disease lung cancer (YES/NO)

# Problem Formulation / Statistical Learning Algorithms Used

This project aims to analyze the relationship between lifestyle choices, demographic attributes, and lung cancer occurrence to build a predictive classification model for future occurrence of lung cancer. These findings will help in understanding which factors contribute most to the development of lung cancer, helping individuals better understand their risk of developing lung cancer and how they can mitigate these risks. 

To achieve this goal, we will employ multiple statistical learning algorithms today to classify and predict whether an individual is at risk of lung cancer in the future. We will utilize logistic regression, Elastic Net Regression, K-Nearest Neighbors, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Random Forest, all of which will be discussed and elaborated on further in the project. These models will be tuned with k-Fold Cross Validation and finally evaluated using ROC AUC scores, to which we will pick the best-performing model for predicting lung disease risk to test. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
suppressPackageStartupMessages({
  library(caret)
  library(ggplot2)
  library(dplyr)
  library(glmnet)
  library(corrplot)
  library(reshape2)
  library(car)
  library(rsample)
  library(tidymodels)
  library(discrim)
  library(ISLR)
  library(tree)
  library(randomForest)
  library(gbm)
  library(ROCR)
  library(knitr)
})
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
lung_data <- read.csv("/Users/faithju/Desktop/Lung Cancer Dataset.csv")
```


# Exploratory Data Analysis and Visualization 

## Cleaning/Tidying Data 

First we load the dataset from a CSV file; cleaning the variable names was unnecessary for this dataset since all the variable names were very consistent and readable. 

Since all variables except for ENERGY_LEVEL, OXYGEN_SATURATION, and PULMONARY_DISEASE were already in a binary 0/1 format, no additional transformations were needed for them. We then converted PULMONARY_DISEASE, which was originally encoded as "YES/NO", into a factor to properly represent it as a categorical variable for machine learning models. Next, we standardized the numeric variables ENERGY_LEVEL and OXYGEN_SATURATION using centering and scaling to ensure they contribute equally to model performance.

Finally, we merged the standardized variables back into the dataset, ensuring that the categorical and binary features remained intact while optimizing the numerical data for analysis.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#convert categorical variable to factor 
lung_data$PULMONARY_DISEASE <- as.factor(lung_data$PULMONARY_DISEASE)

#standardize numeric features 
preProcValues <- preProcess(lung_data[, c("ENERGY_LEVEL", "OXYGEN_SATURATION")],
                            method = c("center", "scale"))
lung_data_scaled <- predict(preProcValues, lung_data)

#merge scaled data back into dataset
lung_data_scaled$PULMONARY_DISEASE <- lung_data$PULMONARY_DISEASE
```

Next, we will check if the dataset has missing values. 

```{r, echo=FALSE}
#check for missing values
missing_values <- data.frame(
  colSums(is.na(lung_data))
)

kable(missing_values, col.names=c("Missing Values"))
```

There are no missing values present, so we will proceed. 

## Pollution Exposure vs Lung Cancer - Comparison 

Exposure to air pollution is a significant environmental risk factor that has been linked to various respiratory and cardiovascular diseases, including lung cancer. Pollutants such as fine particulate matter, nitrogen dioxide, and other airborne toxins can cause inflammation and DNA damage, increasing the likelihood of cancerous cell development. With industrial emissions, vehicle exhaust, and poor air quality becoming growing concerns in urban areas, understanding the long-term effects of pollution exposure on lung health is crucial. 

Now we will examine the relationship between pollution exposure levels and the occurrence of lung cancer to see whether individuals exposed to higher pollution levels are at an increased risk of developing the disease.
This bar plot compares the number of individuals diagnosed with and without lung cancer across different levels of pollution exposure. The two categories, low exposure and high exposure, show distinct differences in lung cancer occurrence. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.height=3, fig.width=5}
ggplot(lung_data, aes(x = as.factor(EXPOSURE_TO_POLLUTION), fill = PULMONARY_DISEASE)) +
  geom_bar(position = "dodge") +  
  scale_x_discrete(labels = c("0" = "Low Exposure", "1" = "High Exposure")) +  
  labs(title = "Pollution Exposure vs. Lung Cancer Diagnosis", 
       x = "Pollution Exposure", 
       y = "Count")
```

From the plot, we observe that among individuals with low pollution exposure, the majority do not have lung cancer, though a portion of the group has been diagnosed with the disease. However, in the high pollution exposure group, the number of lung cancer cases is noticeably higher, suggesting a potential correlation between pollution exposure and lung cancer occurrence. This visualization indicates that individuals exposed to higher levels of pollution are more likely to develop lung cancer compared to those with low exposure. 


## Smokers vs. Lung Cancer - Comparison 

Smoking is a well-documented risk factor for numerous health conditions, with lung cancer being one of the most severe consequences. The chemicals found in cigarettes and other nicotine products can cause DNA mutations, increasing the likelihood of cancerous cell growth. Prolonged smoking exposure damages lung tissues, weakens immune responses, and contributes to chronic inflammation, creating an environment conducive to tumor development. As smoking continues to be a widespread habit, fueled by the accessibility of traditional cigarettes, nicotine vaporizers, and pouches, the risks associated with tobacco consumption remain a significant public health concern. Despite ongoing efforts to raise awareness and implement smoking cessation programs, nicotine addiction continues to increase the rate of tobacco use, particularly among young adults. Understanding smokingâ€™s impact on lung health is crucial in developing strategies for prevention, early detection, and intervention to reduce the prevalence of smoking-related diseases, including lung cancer.

This stacked proportion bar chart visualizes the relationship between smoking status and the occurrence of pulmonary disease (lung cancer). The two bars represent non-smokers and smokers, with the proportions of individuals with (YES) and without (NO) pulmonary disease displayed within each category.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=6}
ggplot(lung_data, aes(x = as.factor(SMOKING), fill = PULMONARY_DISEASE)) +
  geom_bar(position = "fill") +  # Makes it proportional
  scale_y_continuous(labels = scales::percent) +  
  scale_x_discrete(labels = c("0" = "Non-Smoker", "1" = "Smoker")) +  
  labs(title = "Proportion of Lung Cancer by Smoking Status", 
       x = "Smoking Status", 
       y = "Proportion")
```

From the plot, we can observe a notable difference in lung cancer occurrence between smokers and non-smokers. Among non-smokers, the majority do not have lung cancer, and among smokers, the proportion of lung cancer occurrence is significantly higher. This suggests a strong association between smoking and lung cancer, as a greater proportion of smokers are diagnosed with the disease compared to non-smokers. 

We will now move on to create a correlation plot to examine the relationships between our predictor variables and the response variable.

## Variable Correlation Plot

To further explore and visualize the relationships between key predictors and our response variable, we will create a variable correlation plot. Since our dataset contains a mix of binary (0/1) categorical variables and continuous numerical variables, we have to convert categorical variables into a numeric format first for this correlation matrix to provide meaningful insights. 


```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Convert categorical variable (YES/NO) to numeric (1="YES", 0="NO")
lung_data_numeric <- lung_data %>%
  mutate(PULMONARY_DISEASE = ifelse(PULMONARY_DISEASE == "YES", 1, 0)) %>%
  mutate(across(where(is.factor), as.numeric))

cor_matrix <- cor(lung_data_numeric, use = "complete.obs")
cor_melted <- melt(cor_matrix)

ggplot(cor_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1,1), name="Correlation") + 
  labs(title = "Variable Correlation Heatmap") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=10),
        axis.text.y = element_text(size=10)) + 
  coord_fixed()
```

This correlation heatmap provides a visualization of the relationships between different variables in the data set. The color intensity indicates the strength and direction of correlations, with red shades representing positive correlations, blue shades representing negative correlations, and white indicating little to no correlation. The strongest correlations are observed along the diagonal, which is expected since each variable is perfectly correlated with itself.

Beyond the diagonal, it is evident there are some moderate correlations between variables and lung disease. For example, there appears to be a positive correlation between a family history of smoking and lung disease, as well as other variables (exposure to pollution, immune weakness, having breathing issues, throat discomfort) and lung disease. These relationships suggest that lifestyle and environmental factors may play a significant role in the likelihood of developing lung disease. However, most of the variables exhibit weak correlations, implying that many of them may act independently rather than being highly interrelated. 

After looking at this plot, further statistical analysis is clearly necessary to determine whether these correlations are statistically significant. 

Now that we have a general idea of how our variables impact whether one has lung cancer or not, we can create a train/test split. 

# Training and Testing 

To ensure our model is both well-trained and effectively evaluated, we split our dataset into a training set (75%) and a testing set (25%). This will provide enough data for training as well as reserve sufficient data to evaluate model performance on unseen data. 

We first set a seed to ensure reproducbility, and the initial_split() function is then used to divide the data set. We stratify on our response variable, and receive subsets from the training() and testing() functions.   

```{r}
#Sample 75% observations as training data
set.seed(123)
train = sample(nrow(lung_data), 0.75*nrow(lung_data)) 
train.lungs = lung_data[train,]

#Leave the rest as test data
test.lungs = lung_data[-train,]
```

Here are the dimensions of our training and test sets. 

```{r}
dim(train.lungs)
dim(test.lungs)
```

## K-Fold Cross Validation 

K-Fold Cross-Validation is a model validaion technique used to evalute model performance while making the most of available data. It splits the training data into K subsets ("folds"), allowing the model to be trained and tested multiple times on different portions of data. This reduces variance in performance estimates and makes model evaluation more reliable. 

Here we use vfold_cv to create 10 folds from the training set, which means the model is trained on 9 folds and tested on the remaining 1 fold, with this process repeating 10 times. Each fold serves as a validation set once. At the end, the results are averaged. Stratifying the data based on the response variable PULMONARY_DISEASE ensures that each fold has the same proportion of YES/NO cases, preventing class imbalance. 

```{r}
lung_folds <- vfold_cv(train.lungs, v = 10, strata = PULMONARY_DISEASE)
lung_folds
```

# Fitted models - Evaluation, Comparison and Presentation 

Now we will begin building our models. Throughout this project, ROC AUC will be our performance metric because it provides a comprehensive measure of model performance, specifically in classification problems. The AUC score measures the model's ability to distinguish between classes, with a value near 1.0 indicating strong predictive performance and a value around 0.5 suggesting that the model performs no better than random guessing. Overall, an ROC AUC performance metric ensures we balances sensitivity and specificity in our model. 

# Model Overview 

## Set Recipe 

Setting a recipe allows us to preprocess the data before training the model. It specifies PULMONARY_DISEASE as the response and ensures all variables are on a similar scale to improve our model's performance and stability. 

```{r}
lung_recipe <- recipe(PULMONARY_DISEASE ~ ., data = train.lungs) %>%
  step_normalize(all_numeric_predictors())
```

We will set up models for Logistic Regression, Elastic Net Regression, K-Nearest Neighbor, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Random Forest, compare the models based off of ROC AUC, and pick the best one to test. 

# Models and Visualization 

To ensure our models do not encounter multicollinearity errors, we will now use the vif() function to calculate the Variance Inflation Factor for each predictor variable in the fitted Logistic Regression Model. 

## Logistic Regression 

Before fitting the logistic regression model,  it is important to assess whether any of the predictor variables are highly correlated with each other. To detect multicollinearity, we calculate the VIF (Variance Inflation Factor) for each predictor. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
log_reg_model <- glm(PULMONARY_DISEASE ~ .,
           data = lung_data_numeric, family = "binomial")  #logistic regression 
vif_values <- vif(log_reg_model)
print(vif_values)

log_reg_preds <- predict(log_reg_model, newdata = train.lungs, type = "response")
log_reg_results <- tibble(
  PULMONARY_DISEASE = train.lungs$PULMONARY_DISEASE,  
  .pred_YES = log_reg_preds  
)
```

These values indicate the degree of multicollinearity. Since all of these values are fairly low, this indicates low multicollinearity between the predictor variables, suggesting that each variable provides independent information to this model. This is ideal for logistic regression stability. 

Now we'll plot the ROC curve for the logistic regression model. 


```{r, echo=FALSE, message=FALSE, warning=FALSE}
#plot roc curve
log_reg_roc <- roc_curve(log_reg_results, truth = PULMONARY_DISEASE, .pred_YES, event_level = "second")
autoplot(log_reg_roc) +
  labs(title = "ROC Curve - Logistic Regression",
       x = "1 - Specificity (False Positive Rate)",
       y = "Sensitivity (True Positive Rate)")
```

This model shows strong classficiation performance. The steep initial rise suggests that it is good at capturing true positives while keeping false positives low. The curve remains well above the reference line, and suggests a high AUC score. 

# Elastic Net Regression 

Now we will tune the penalty and balance L1 (Lasso) and L2 (Ridge) regularization through Elastic Net Regression. This model will remove less important variables and shrink coefficients. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidymodels)
elastic_net <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

elasticnet_wflow <- workflow() %>% 
  add_model(elastic_net) %>% 
  add_recipe(lung_recipe)

elastic_grid <- grid_regular(penalty(), mixture(range = c(0, 1)), levels = 10)
tuned_elasticnet <- tune_grid(elasticnet_wflow, resamples = lung_folds, grid = elastic_grid)

best_elastic_net <- select_best(tuned_elasticnet, metric = "roc_auc")

elastic_net_final <- finalize_workflow(elasticnet_wflow, best_elastic_net)
elastic_net_final_fit <- fit(elastic_net_final, data = train.lungs)

elastic_net_preds <- predict(elastic_net_final_fit, new_data = train.lungs, type = "prob")

show_best(tuned_elasticnet, metric= "roc_auc")
```

After running the elastic net regression model, we obtain a consistently high ROC AUC score of approximately 0.923 across multiple tuning configurations. The penalty values (strength of regularization) range from 4.64e-04 to 2.15e-0.7, while the mixture parameter, determining the balance between L1 and L2 regularization, is predominantly close to 1. This suggests that the model benefits more from L1 regularization, leading to feature selection as it shrinks some coefficients to zero. The best penalty value here is 4.64e-04, as it corresponds to the highest mean ROC AUC score of 0.9232. 


# K Nearest Neighbor

K-Nearest Neighbors (KNN) is a classification algorithm that predicts the class of a new observation by considering the majority class among its K nearest neighbors in the feature space. It relies on distance based similarity. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knn <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>%
  set_mode("classification")

lung_recipe <- recipe(PULMONARY_DISEASE ~ ., data = train.lungs) %>%
  step_normalize(all_numeric_predictors())

knn_wkflow <- workflow()%>%
  add_recipe(lung_recipe) %>%
  add_model(knn)

knn_grid <- grid_regular(neighbors(range = c(1, 10)),
                         levels = 10)
knn_fit <- tune_grid(
  knn_wkflow,
  resamples = lung_folds, 
  grid = knn_grid
)

best_knn <- select_best(knn_fit, metric="roc_auc")

knn_final <- finalize_workflow(knn_wkflow, best_knn)
knn_final_fit <- fit(knn_final, data = train.lungs)

#plot 
autoplot(knn_fit, metric = "roc_auc")
```

This plot shows the relationship between the number of nearest neighbors and the ROC AUC score in a KNN classification model. The highest ROC AUC appears to be at k=10, reaching slightly above 0.90. This indicates the model is performing very well and has a strong ability to distinguish between individuals with and without lung cancer. The steady increase in ROC AUC as k increases indicates that incorporating more neighbors helps stabilize predictions, reducing the risk of overfitting to any noise in the data. KNN does suffer from the Curse of Dimensionality, and fails when there are too many predictors. 

# Linear Discriminant Analysis

Now we will utilize Linear Discriminant Analysis (LDA), a supervised classficiation technique that finds a linear combination of features to maximize separation between classes. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#LDA
lda <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

#workflow for LDA
lda_workflow <- workflow() %>%
  add_model(lda) %>%
  add_recipe(lung_recipe)

lda_fit <- fit(lda_workflow, train.lungs)
predict(lda_fit, new_data = train.lungs, type="prob")

lda_kfold_fit <- fit_resamples(lda_workflow, lung_folds, control = control_grid(save_pred = TRUE))
collect_metrics(lda_kfold_fit)

lung_roc_lda <- augment(lda_fit, train.lungs)

lung_roc_lda %>%
  roc_curve(PULMONARY_DISEASE, .pred_YES, event_level = "second") %>%
  autoplot()
```

This model performs well, with the curve staying high above the diagonal line, showing strong classification performance. It rises sharply towards high sensitivity values early, indicating the model successfully identifies true positive cases. The AUC score is high (will calculate later in the project), indicating LDA is a reliable model. 

# Quadratic Discriminant Analysis 

Quadratic Discriminant Analysis (QDA) is a classification technique that models the decision boundary between classes by allowing for different covariance structures for each class. Our QDA model will predict the likelihood of developing lung cancer. QDA helps separate the classes more effectively in this case. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
qda <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda) %>% 
  add_recipe(lung_recipe)

qda_fit <- fit(qda_wkflow, train.lungs)
predict(qda_fit, new_data = train.lungs, type="prob")


qda_kfold_fit <- fit_resamples(qda_wkflow, lung_folds, control = control_grid(save_pred = TRUE))
collect_metrics(qda_kfold_fit)

lung_roc_qda <- augment(qda_fit, train.lungs)

lung_roc_qda %>%
  roc_curve(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  autoplot()
```


Our QDA model performs well, showing a strong classificatio performance. The curve rises sharply at the top-left corner, showing high sensitivity and specificity. It stays well above the diagonal reference line, and the AUC is very close to 1 (which we will calculate later). 


# Random Forest 

We will tune our hyper perammeter m using the rule of thumb for classification problems \( m = \sqrt{p} \). Since we have 17 predictors, \( m \approx 4 \).

```{r, fig.height=3.5, fig.width=7, fig.align='center', cache=TRUE, echo=FALSE}
rf.lungs = randomForest(PULMONARY_DISEASE ~ ., data=train.lungs,
                           mtry=4, importance=TRUE)
rf.lungs
plot(rf.lungs)

#calculating classification error rate?
yhat.rf = predict(rf.lungs, newdata = test.lungs, type="class")
test.rf.err = mean(yhat.rf != test.lungs$PULMONARY_DISEASE) 
test.rf.err

rf_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_formula(PULMONARY_DISEASE ~ .)

rf_grid <- grid_regular(mtry(range = c(2, 14)), trees(range = c(50, 300)), min_n(range = c(2, 10)), levels = 3)

rf_tuned <- tune_grid(
  rf_workflow,
  resamples = lung_folds,
  grid = rf_grid,
  metrics = metric_set(roc_auc) 
)

best_rf_params <- select_best(rf_tuned, metric = "roc_auc")

rf_final_workflow <- finalize_workflow(rf_workflow, best_rf_params)

rf_final_fit <- fit(rf_final_workflow, data = train.lungs)

```

This plot represents the error rate convergence of the Random Forest model, with the x-axis representing the amount of trees (ranging from 0-500) and the y-axis representing the error rate. The black line represents the OOB error rate, the red dashed line represents the error rate for "NO", and the green dashed line represents the error rate for "YES". We can observe that as the number of trees increases, the error rate sharply drops within the first 50 trees, and flattens out around 100 trees. The model performs well, as the error rate is relatively low, but increasing trees above 100 does not seem to improve accuracy. 


Here we can visualize how important each variable is to the Random Forest model. 

```{r, echo=FALSE}
varImpPlot(rf.lungs, sort=T,
           main="Variable Importance", n.var=17)
```

This allows us to visualize the most influential predictors in the Random Forest model used to classify whether someone has lung disease or not. The plot has two metrics- the Mean Decrease in Accuracy and the Mean Decrease in Gini Index. The left panel measures how much the model's accuracy drops when a specific variable is removed, with the higher the value the better. The top few most important  variables are throat discomfort, breathing issues, and smoking, suggesting that respiratory symptoms play a big role in identifying lung disease. The right panel, the Mean Decrease in Gini Index, measures how well a variable helps the model separate individuals with and without lung disease. Here, smoking is the most significant factor, followed by energy levels and throat discomfort, indicating that lifestyle factors and early symptoms are important in distinguishing between cases. 



# Model Evaluation and Comparison 

We will now compute all the final ROC AUC scores to pick the best model. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#store roc-auc results to variables

log_reg_auc <- roc_auc(log_reg_results, PULMONARY_DISEASE,
                       .pred_YES, event_level = "second") %>%
  pull(.estimate)

knn_auc <- augment(knn_final_fit, new_data = train.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)

elastic_net_auc <- augment(elastic_net_final_fit, new_data = train.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)


lda_auc <- augment(lda_fit, new_data=train.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)

qda_auc <- augment(qda_fit, new_data = train.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)

rf_auc <- augment(rf_final_fit, new_data = train.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)
```

## Final ROC AUC Results 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#place resulting auc into a dataframe
auc_results <- data.frame(
  log_reg_auc,
  knn_auc,
  elastic_net_auc,
  lda_auc,
  qda_auc,
  rf_auc
)

kable(auc_results, 
      col.names=c("Logistic Regression", "KNN", "Elastic Net", "LDA", "QDA", "Random Forest"),
      caption="AUC For Each Model's ROC Curve")
```

# Selecting Model 

Now that we've compiled a table of AUC scores for each model's ROC curve, we are able to identify that our Random Forest model performed the best overall, with the highest AUC score of 0.9891827. This model was fitted on the training data, meaning that we must examine how it performs on our testing data. 

# Testing Our Model 

Now, we will utilize our Random Forest model to fit our testing data and reveal its performance in predicting if an individual has lung cancer. 

```{r}
has_lung_cancer <- data.frame(
  AGE = 32, 
  GENDER = 1, 
  SMOKING = 1, 
  FINGER_DISCOLORATION = 1, 
  MENTAL_STRESS = 0,
  EXPOSURE_TO_POLLUTION = 1,
  LONG_TERM_ILLNESS = 0,
  ENERGY_LEVEL = 57.51082665146830, 
  IMMUNE_WEAKNESS = 0,
  BREATHING_ISSUE = 1,
  ALCOHOL_CONSUMPTION =1, 
  THROAT_DISCOMFORT = 1,
  OXYGEN_SATURATION = 95.19584170883560, 
  CHEST_TIGHTNESS = 1,
  FAMILY_HISTORY = 0, 
  SMOKING_FAMILY_HISTORY = 0,
  STRESS_IMMUNE = 0
)

predict(rf_final_fit, has_lung_cancer, type = "class")
```
We can see that the model has successfully classified this individual as having lung cancer. 


```{r}
no_lung_cancer <- data.frame(
  AGE = 52, 
  GENDER = 0, 
  SMOKING = 0, 
  FINGER_DISCOLORATION = 0, 
  MENTAL_STRESS = 1,
  EXPOSURE_TO_POLLUTION = 1,
  LONG_TERM_ILLNESS = 1,
  ENERGY_LEVEL = 58.319318845729600, 
  IMMUNE_WEAKNESS = 0,
  BREATHING_ISSUE = 1,
  ALCOHOL_CONSUMPTION =0, 
  THROAT_DISCOMFORT = 1,
  OXYGEN_SATURATION = 96.05509653586510, 
  CHEST_TIGHTNESS = 0,
  FAMILY_HISTORY = 0, 
  SMOKING_FAMILY_HISTORY = 0,
  STRESS_IMMUNE = 0
)

predict(rf_final_fit, no_lung_cancer, type = "class")
```

The model has successfully classified this non-cancerous individual as not having lung cancer.


Now we test the generalization capabilities of our model on the test data and compute the ROC AUC score to evaluate its ability to distinguish between cancerous and non-cancerous individuals. 

```{r, echo=FALSE}
rf_auc <- augment(rf_final_fit, new_data = test.lungs) %>%
  roc_auc(PULMONARY_DISEASE, .pred_YES, event_level="second") %>%
  pull(.estimate)

rf_auc
```

Our Random Forest model was able to predict lung-cancerous individuals in our data with a 92.9% ROC AUC score! 

```{r, echo=FALSE}
rf_class_predictions <- predict(rf_final_fit, new_data = test.lungs, type = "class")

rf_accuracy_results <- test.lungs %>%
  mutate(.pred_class = rf_class_predictions$.pred_class)

rf_test_accuracy <- accuracy(rf_accuracy_results, truth = PULMONARY_DISEASE, estimate = .pred_class) %>%
  pull(.estimate)

rf_test_accuracy
```

To delve deeper, we computed the accuracy rate of correctly classified cases. Our Random Forest model was able to predict lung-cancerous individuals in our data with a 91.36% accuracy rate. 

# Conclusions / Future Directions for Improvement 

This project explored the relationships between many demographic, lifestyle, and physiological factors in predicting lung disease. After extensive testing of 6 models, we discovered that our Random Forest model performed the best in predicting lung-cancerous individuals, with the highest ROC AUC score of 0.989. 

Several variables emerged as particularly influential in predicting lung disease risk. Smoking, breathing issues, and throat discomfort were among the most significant predictors across all models, reinforcing published medical studies that identify smoking and respiratory systems as major risk factors for lung cancer. Other variables like exposure to pollution and having smoking in the family history were also strongly correlated with a positive lung cancer disease diagnosis. This highlights the importance of environmental and genetic factors as well in determining risk for lung cancer. 

The findings of this project have significant real-world applications in preventative medicine and public health policy. Identifying high-risk individuals early on enables targeted screening programs, lifestyle interventions, and treatment plans, all of which could improve patient outcomes and survival rates compared to late-stage diagnoses. Models like the one developed in this study can serve as tools for those in the medical field to help identify at-risk patients before symptoms become more severe. Leveraging data science for early intervention can also significantly impact public health efforts. 

There are definitely areas for future improvement; one limitation of this study was that we only had 5000 observations. If we had more data, we could build a model that can generalize better. We could also explore the use of deep learning or other predictor variables like CT scan results. We could also explore the usage of other evaluation metrics to generate a better understanding of model performance. 

In conclusion, our project illustrates the impact of machine learning in the medical field, where many can leverage data-driven insights to develop healthcare initiatives, improving long-term health outcomes and reducing lung cancer mortality rates. 






